flowchart TB
  subgraph CFG["Configuration"]
    CM["ConfigManager"]
    YAML["YAML Parser (yaml)"]
    FSIO["File IO (fs&amp;&num;45;extra)"]
  end
  subgraph CORE["Core Processing"]
    CLI["CLI Entry (src/cli.ts)"]
    FD["FileDiscovery (glob/fs)"]
    TC["TokenCalculator (estimation)"]
    BM["BucketManager (token bucketing &amp;amp; splitting)"]
    TYPES["Types: FileInfo, ProcessingBucket, Config"]
  end
  subgraph LLM["LLM Layer"]
    LLMIF["LLMInterface (abstract)"]
    OAIC["OpenAIClient (provider)"]
    OAI["OpenAI SDK (openai)"]
  end
  subgraph OUT["Output &amp;amp; Rendering"]
    MG["MermaidGenerator (merge/sanitize/validate)"]
    WRITER["Mermaid Writer"]
    SM["StateManager (progress/checkpoints)"]
    OUTFILE(("Output Diagram: repo.mermaid"))
  end
  subgraph GIT["GitHub Integration"]
    GAH["GitHubActionHandler (workflow)"]
    GHC["GitHubClient (Octokit)"]
    GH["GitHub Platform"]
  end
  subgraph TESTS["Tests"]
    IT["Integration Test (full&amp;&num;45;integration.test.js)"]
    TCFG["Unit Test: ConfigManager"]
    TFD["Unit Test: FileDiscovery"]
    TTC["Unit Test: TokenCalculator"]
    TBM["Unit Test: BucketManager"]
    TLLMIF["Unit Test: LLMInterface"]
    TOAIC["Unit Test: OpenAIClient"]
  end
  %% Orchestration & data flow
  CLI --> CM
  CM --> YAML
  CM --> FSIO
  CLI --> FD
  FD --> TC
  CLI --> BM
  BM --> TC
  BM --> TYPES
  CLI --> MG
  MG --> OAIC
  LLMIF --> OAIC
  OAIC --> OAI
  MG --> WRITER
  OAIC --> WRITER
  WRITER --> FSIO
  WRITER --> OUTFILE
  %% Filesystem interactions
  FD --> FSIO
  SM --> FSIO
  %% GitHub Action workflow
  GAH --> GHC
  GAH --> FD
  GAH --> BM
  GAH --> OAIC
  GAH --> MG
  GHC --> GH
  %% Tests coverage
  IT --> CLI
  IT --> MG
  IT --> WRITER
  IT --> OUTFILE
  TCFG --> CM
  TFD --> FD
  TTC --> TC
  TBM --> BM
  TLLMIF --> LLMIF
  TOAIC --> OAIC
  %% Styles
  classDef tests fill:#e17055,stroke:#ffffff,stroke-width:2px
  classDef config fill:#fdcb6e,stroke:#ffffff,stroke-width:2px
  classDef core fill:#0984e3,stroke:#ffffff,stroke-width:2px
  classDef llm fill:#55efc4,stroke:#ffffff,stroke-width:2px
  classDef output fill:#6c5ce7,stroke:#ffffff,stroke-width:2px
  %% Class assignments
  class CM,YAML,FSIO config
  class CLI,FD,TC,BM,TYPES core
  class LLMIF,OAIC,OAI llm
  class MG,WRITER,SM,OUTFILE,GAH,GHC,GH output
  class IT,TCFG,TFD,TTC,TBM,TLLMIF,TOAIC tests

<!--
Generated Mermaid Diagram
========================

Summary:
The codebase implements a CLI-driven system that analyzes a repository, batches files by estimated token usage, and uses an LLM to generate an architectural summary and a GitHub‑compatible Mermaid diagram.

Key architectural layers and components:

1) CLI Orchestration (core)
- The CLI entrypoint (src/cli.ts, inferred from tests) coordinates the end-to-end flow: loads configuration, discovers files, estimates tokens, groups files into buckets, invokes the LLM for summaries/diagrams, merges/repairs fragments if needed, and writes the final Mermaid output file.

2) Configuration subsystem (config)
- ConfigManager (src/ConfigManager, tested) loads YAML config via fs-extra and yaml, merges CLI args with file config, and provides validation and defaults. It supports saving configs and creating sample configs. It exposes provider/model defaults, supported file types/providers, and respects CLI overrides.
- Behavior from tests:
  - loadConfig: falls back to defaults if no file or read error; merges CLI args over file config.
  - saveConfig: writes YAML; errors are surfaced with clear messages.
  - validateConfig: verifies file_types, llm.provider, llm.max_tokens (>0), llm.temperature in [0,2], etc.
  - Utility getters: supported file types/providers, default models, config path management.

3) Core processing (core)
- BucketManager (src/core/BucketManager, tested) groups files into ProcessingBuckets based on token limits and a readiness threshold. Features include:
  - createBuckets: token-aware grouping respecting a per-bucket soft limit and a global hard limit (~400k tokens in tests).
  - isBucketReady/isBucketAtCapacity/getBucketUtilization: readiness and utilization checks.
  - add/remove files, split over-capacity buckets, and merge when feasible.
  - optimizeBuckets: improves distribution under the token cap.
  - Skipped file tracking: files exceeding hard limits are skipped with a summary; supports clearing this state.
  - Statistics: totals, averages, and utilization across buckets.

4) LLM integration (llm)
- LLMInterface (abstract, inferred): provides common behaviors such as prompt construction (createSystemPrompt/createUserPrompt), response parsing (parseResponse), and Mermaid cleanup (cleanMermaidContent). It standardizes provider behavior and supports the Strategy pattern for different providers.
- OpenAIClient (src/llm/OpenAIClient.ts and compiled JS variant): implements the LLM provider using the official OpenAI SDK.
  - processBucket: produces {summary, mermaid_content, tokens_used}. Uses Chat Completions for GPT‑4/3.5 and the Responses API path for a GPT‑5 placeholder, parsing responses accordingly.
  - generateSummary: produces a concise architecture summary for a set of files.
  - generateMermaid: creates a Mermaid diagram from a summary and a subset of key files; returns cleaned, GitHub‑compatible Mermaid.
  - mergeOrRepairMermaid (TS version): merges multiple Mermaid fragments into a single, GitHub‑compatible diagram, enforcing strict syntax rules (no edge labels like -->|label|, permitted diagram types only, quoted labels with spaces, escaping special characters, etc.).
  - validateConnection/getAvailableModels/testConnection: diagnostics to list models and validate connectivity.
  - estimateCost: rough token-based cost estimation per model family.

5) Output and rendering (output)
- Final Mermaid content is written to an output file (e.g., repo.mermaid) using fs-extra. The system prioritizes GitHub Mermaid syntax compatibility.

6) Tests (tests)
- Unit tests cover ConfigManager and BucketManager behaviors (loading/merging/validation/saving for config; bucketing, capacity, merging/splitting, stats, optimization, and skipped files for buckets).
- Integration test scaffolds a miniature TypeScript project (services, models, database, UI component), runs the CLI via ts-node, verifies diagram generation, handles empty directories, specific file lists, and dry-run behavior. It guards on OPENAI_API_KEY presence and asserts Mermaid-like content is produced.

Notable design considerations and patterns:
- Strategy/Adapter for LLM providers via LLMInterface and OpenAIClient. The OpenAIClient adapts SDK differences (Chat Completions vs. Responses API). Error handling wraps SDK exceptions with clear messages.
- Token-aware batching enables scalability for large codebases. The hard-limit safeguard avoids API failures, while optimize/split/merge operations improve utilization.
- Configuration merging prioritizes CLI args, enabling reproducible and parameterized runs. Validation proactively guards misconfiguration.
- Mermaid compatibility rules are embedded in prompts and cleanup, targeting GitHub’s stricter Mermaid support.

Data flow overview:
- CLI loads/validates config → discovers and parses files → estimates tokens → BucketManager creates optimized buckets → per bucket, OpenAIClient generates summaries and Mermaid fragments → optional merge/repair to a single diagram → output file is written (and optionally published to GitHub PR paths as indicated by config fields, though the publishing logic is not shown in provided files).

--- Additional Analysis ---

Architecture overview:

The system is a CLI-driven pipeline that discovers code files, buckets them by estimated token usage, invokes an LLM to synthesize an architectural summary and Mermaid diagram fragments, merges and sanitizes the diagram for GitHub compatibility, and writes the final output. It can optionally run as a GitHub Action to create/update a diagram and open or update a pull request.

Key layers and responsibilities:

1) CLI Orchestration (core)
- src/cli.ts coordinates the end-to-end workflow: parses CLI args, loads/validates configuration, discovers files, tokenizes and splits large files, creates token-aware buckets, initializes the LLM client and output generator, processes buckets, writes the Mermaid diagram, and persists processing state.
- Uses: ConfigManager, FileDiscovery, BucketManager, OpenAIClient (via the LLMInterface abstraction), MermaidGenerator, and StateManager. It can optionally integrate with GitHub via GitHubActionHandler (out-of-band to the CLI flow).

2) Configuration subsystem (config)
- ConfigManager (src/ConfigManager.ts, plus compiled JS variant) loads YAML configuration, merges it with CLI arguments, exposes defaults and getters, and validates constraints (e.g., file_types non-empty, llm provider/model present, max_tokens > 0, temperature in [0,2], output path provided). It also supports saving and generating sample configs. The TS default config includes colors and additional_instructions fields; the bundled YAML (mermaid_generator_config.yaml) demonstrates standard defaults and color scheme.
- Note: There are differences between TS and compiled JS defaults (e.g., include_summary true vs false; llm.max_tokens and model values differ across TS/JS). Ensure your build artifacts align with the TypeScript sources to avoid mismatches at runtime.

3) Core processing (core)
- FileDiscovery scans files using glob (recursive) or directory reads (non-recursive), honors exclude_patterns (including wildcard handling), normalizes Windows paths, and produces FileInfo objects with estimated_tokens (roughly content length/4). It can also filter and sort by size/tokens.
- TokenCalculator provides a more nuanced estimation: strips comments/empty lines, estimates per-line tokens, applies token-per-char ratios, and exposes density-based adjustments per file extension. It also estimates prompt/message token usage.
- BucketManager groups FileInfo into ProcessingBuckets under a per-bucket soft limit and a readiness threshold. It tracks a hardLimit safety cap, marks files exceeding the hard limit as skipped, and supports:
  - createBuckets: greedy packing by descending token estimate; respects threshold and hardLimit.
  - isBucketReady/isBucketAtCapacity/getBucketUtilization.
  - add/remove file operations.
  - splitBucket for over-capacity buckets; mergeBuckets when within limits.
  - optimizeBuckets: splits overutilized buckets and merges underutilized ones where feasible.
  - processFilesWithSplitting: splits very large files into line-based chunks with recalculated token estimates to stay within hard limits.
- Important note: The TS implementation uses hardLimit=100k, while the compiled JS sets hardLimit=400k. This discrepancy can alter bucketing, skipped-files behavior, and splitting. Align the build to avoid inconsistent behavior.

4) LLM integration (llm)
- LLMInterface defines the strategy interface for providers and centralizes prompt construction, response parsing, and Mermaid cleanup. It embeds strong instructions for GitHub-compatible Mermaid syntax (no edge labels, only supported diagram types, quote/escape special chars, bracket/parenthesis balance checks). It also carries a configurable color scheme and additional_instructions.
- OpenAIClient (provider implementation) uses the OpenAI SDK (Chat Completions/Responses APIs) to:
  - processBucket: return {summary, mermaid_content, tokens_used}.
  - generateSummary and generateMermaid for separate phases.
  - mergeOrRepairMermaid: merge or repair fragments into a single GitHub-compatible diagram.
  - validateConnection/getAvailableModels/testConnection and estimateCost.

5) Output and rendering (output)
- MermaidGenerator coordinates per-bucket processing with the LLM and accumulates summaries and diagram fragments. It defers final diagram merge/repair to the LLM’s mergeOrRepairMermaid, then sanitizes for GitHub compatibility:
  - Removes unsupported edge labels, quotes labels with spaces/special characters, escapes &, <, >, and normalizes line breaks.
  - validateMermaidSyntax performs basic checks (type presence, bracket/parenthesis balance).
  - Writes the final diagram to the configured file path and optionally appends a summary and skipped-files list as an HTML comment block.
- StateManager persists processing state (JSON file), tracks progress, and can create/load checkpoints with minimal file metadata to resume/inspect runs.

6) GitHub integration (output/integration)
- GitHubClient wraps Octokit for repository operations: create branch, check branch existence, CRUD file contents, list PRs/commits/files, add labels/comments, and test connectivity.
- GitHubActionHandler orchestrates a full action workflow: tests GitHub connection, ensures a target branch, fetches existing Mermaid content, runs the same code-processing pipeline (FileDiscovery → BucketManager → OpenAIClient → MermaidGenerator), writes the diagram to the repo, and creates or updates a PR with labels and status comments.

7) Tests (tests)
- Unit tests cover: ConfigManager, FileDiscovery, TokenCalculator, LLMInterface interface behavior, and OpenAIClient interactions. Integration tests scaffold a mini TypeScript project, run the CLI via ts-node, and verify diagram generation and syntax validation under different scenarios (full run, empty dir handling, specific file lists, dry-run behavior). Tests also mock OpenAI and filesystem/glob for isolation.

Design patterns and decisions:
- Strategy/Adapter for LLM providers via LLMInterface; OpenAIClient adapts SDK differences and provides cost/model utilities.
- Orchestration pattern in the CLI and GitHubActionHandler to coordinate subsystems.
- Token-aware batching with safety hard limits and optimization passes to scale across large repos and avoid API failures.
- Defensive Mermaid handling (sanitize/validate) aimed at GitHub’s stricter Mermaid support.
- Separation of concerns across discovery, estimation, bucketing, LLM interaction, output generation, state persistence, and GitHub publishing.

Data flow summary:
- CLI loads/merges/validates config → FileDiscovery loads files → TokenCalculator estimates tokens → BucketManager splits/optimizes buckets (and large files) → MermaidGenerator processes each bucket via OpenAIClient, accumulates fragments/summaries → LLM merge/repair and local sanitize/validate → file written and state saved → optionally GitHubActionHandler commits and PRs the diagram.


Processing Statistics:
- Total files processed: 42
- Total buckets: 2
- Generated on: 2025-10-05T15:14:34.492Z

-->