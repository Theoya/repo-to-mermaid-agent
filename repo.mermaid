flowchart TD
  %% Nodes
  CLI(["CLI: mermaid-gen / src/cli.ts"]):::config
  ConfigMgr["ConfigManager\n- loadConfig(args)\n- saveConfig()\n- validateConfig()"]:::config
  DecideArgs{Overrides?\nCLI args &gt; file}:::config
  YAML[[yaml parser]]:::config
  FS[(fs-extra)]:::config
  GH["GitHub Integration\n(Octokit)"]:::config
  Scanner["File Scanner / Glob"]:::config
  Types[["Types:\nFileInfo,\nProcessingBucket,\nProcessingState"]]:::core
  BucketMgr["BucketManager\n- createBuckets()\n- optimizeBuckets()\n- stats / skipped"]:::core
  LLMIF["LLMInterface"]:::llm
  OpenAI["OpenAIClient"]:::llm
  OpenAIAPI((OpenAI API)):::llm
  MermaidGen[["MermaidGenerator\n- processBuckets()\n- merge/sanitize\n- validateMermaid"]]:::output
  OutputFile((repo.mermaid)):::output
  TestsBucket(((Unit: BucketManager.test))):::tests
  TestsConfig(((Unit: ConfigManager.test))):::tests
  TestsIntegration(((Integration: full-integration.test))):::tests
  %% Edges: Configuration
  %% Edges: File collection and typing
  %% Edges: Bucketing
  %% Edges: LLM selection and processing
  %% Edges: Mermaid generation and output
  %% Optional GitHub integration
  CLI -.->|"optional PR commit"| GH
  %% Tests
  %% Styles
  classDef tests fill:#e17055,stroke:#ffffff,stroke-width:2px
  classDef config fill:#fdcb6e,stroke:#ffffff,stroke-width:2px
  classDef core fill:#0984e3,stroke:#ffffff,stroke-width:2px
  classDef llm fill:#55efc4,stroke:#ffffff,stroke-width:2px
  classDef output fill:#6c5ce7,stroke:#ffffff,stroke-width:2px
  %% Class assignments
  class CLI,ConfigMgr,DecideArgs,YAML,FS,Scanner,GH config
  class BucketMgr,Types core
  class LLMIF,OpenAI,OpenAIAPI llm
  class MermaidGen,OutputFile output
  class TestsBucket,TestsConfig,TestsIntegration tests
  %% Subgraphs
  subgraph CCFG[CLI & Configuration]
  CLI["CLI: mermaid-gen (src/cli.ts)"]:::config
  ConfigMgr["ConfigManager\n- load/merge/validate\n- defaults + colors\n- save/create sample"]:::config
  DecideArgs{Overrides?\nCLI &gt; file}:::config
  end
  subgraph CORE[Core Processing]
  FileDisc["FileDiscovery\n- glob/readdir\n- filters/ignores\n- estimateTokens"]:::core
  BucketMgr["BucketManager\n- create/optimize/split/merge\n- processFilesWithSplitting\n- skippedFiles"]:::core
  TokenCalc["TokenCalculator\n- line/density-based estimates\n- prompt/message tokens"]:::core
  Glob[[glob]]:::config
  subgraph LLM[LLM Abstraction]
  LLMIF["LLMInterface\n- prompts/parsing\n- cleanMermaid"]:::llm
  OpenAI["OpenAIClient\n- chat/responses API\n- models/cost\n- test/validate"]:::llm
  OpenAIAPI(((OpenAI API))):::llm
  subgraph OUT[Output]
  MermaidGen["MermaidGenerator\n- processBuckets\n- merge/sanitize/validate\n- generateFinalFile"]:::output
  StateMgr["StateManager\n- progress\n- checkpoints\n- merge accumulators"]:::output
  OutputFile(((repo.mermaid))):::output
  subgraph GH[GitHub Automation]
  GHAction["GitHubActionHandler\n- branch/PR orchestration\n- uses pipeline"]:::config
  GHClient["GitHubClient (Octokit)"]:::config
  OctokitAPI(((Octokit REST API))):::config
  subgraph TESTS[Tests]
  TestCfg(((ConfigManager.test))):::tests
  TestFD(((FileDiscovery.test))):::tests
  TestTK(((TokenCalculator.test))):::tests
  TestLLMIF(((LLMInterface.test))):::tests
  TestOpenAI(((OpenAIClient.test))):::tests
  TestInt(((Full Integration))):::tests
  %% Decisions
  DecideSpec{Specific files?}:::config
  DecideRec{Recursive?}:::config
  DecideExisting{Existing Mermaid?}:::config
  DecideBranch{Branch exists?}:::config
  %% CLI/config interactions
  %% Discovery
  %% Bucketing
  %% LLM selection and processing
  %% Output
  StateMgr -. save/load .-> FS
  %% Existing diagram reuse
  %% GitHub workflow (alternative entry)
  class CLI,ConfigMgr,DecideArgs,YAML,FS,Glob,DecideSpec,DecideRec,DecideExisting,DecideBranch,GHAction,GHClient,OctokitAPI config
  class FileDisc,BucketMgr,TokenCalc core
  class MermaidGen,StateMgr,OutputFile output
  class TestCfg,TestFD,TestTK,TestLLMIF,TestOpenAI,TestInt tests
  CLI -->|"load/merge config"| ConfigMgr
  ConfigMgr -->|"parse/stringify"| YAML
  ConfigMgr -->|"read/write / pathExists"| FS
  ConfigMgr --> DecideArgs
  DecideArgs -- yes --> ConfigMgr
  CLI -->|"collect files"| Scanner
  Scanner --> FS
  Scanner --> Types
  CLI -->|"createBuckets(FileInfo[])"| BucketMgr
  BucketMgr --> Types
  CLI -->|"select provider"| LLMIF
  OpenAI -- "extends / implements" --> LLMIF
  MermaidGen -->|"processBucket()"| LLMIF
  OpenAI -->|"chat.completions / responses"| OpenAIAPI
  CLI -->|"process pipeline"| MermaidGen
  BucketMgr -- "buckets + skippedFiles" --> MermaidGen
  MermaidGen -->|"ensureDir / writeFile"| FS
  MermaidGen --> OutputFile
  TestsBucket --> BucketMgr
  TestsConfig --> ConfigMgr
  TestsIntegration --> CLI
  TestsIntegration --> FS
  CLI -->|parse args| ConfigMgr
  ConfigMgr --> YAML
  ConfigMgr --> FS
  CLI --> DecideSpec
  DecideSpec -- yes --> FileDisc
  DecideSpec -- no --> DecideRec
  DecideRec -- yes (glob) --> FileDisc
  DecideRec -- no (dir scan) --> FileDisc
  FileDisc --> Glob
  FileDisc --> FS
  CLI -->|createBuckets| BucketMgr
  BucketMgr -->|token calc| TokenCalc
  BucketMgr -- skippedFiles --> MermaidGen
  CLI -->|select provider| LLMIF
  OpenAI -- implements --> LLMIF
  MermaidGen -->|processBucket()| LLMIF
  OpenAI --> OpenAIAPI
  CLI --> MermaidGen
  MermaidGen -->|writeFile| FS
  CLI --> StateMgr
  CLI --> DecideExisting
  DecideExisting -- yes -->|load via fs| FS
  DecideExisting -- yes --> MermaidGen
  GHAction -->|test/get repo| GHClient
  GHClient --> OctokitAPI
  GHAction --> DecideBranch
  DecideBranch -- no -->|create branch| GHClient
  GHAction -->|discover| FileDisc
  GHAction -->|bucket| BucketMgr
  GHAction -->|LLM| OpenAI
  GHAction -->|generate| MermaidGen
  GHAction -->|create/update file| GHClient
  TestCfg --> ConfigMgr
  TestFD --> FileDisc
  TestTK --> TokenCalc
  TestLLMIF --> LLMIF
  TestOpenAI --> OpenAI
  TestInt --> CLI
  TestInt --> FS


<!--
Generated Mermaid Diagram
========================

Summary:
Overview
The repository implements a CLI-driven pipeline to analyze a codebase, summarize its architecture using an LLM, and generate a sanitized, merged Mermaid diagram of the system. The key architectural elements are a configuration manager, a token-aware bucket manager for scaling across larger codebases, pluggable LLM clients behind a common interface, and an output-focused Mermaid generator that merges/validates/sanitizes diagrams and writes the final .mermaid file.

Core Workflow
1) CLI Orchestration (src/cli.ts implied by tests):
- Parses CLI arguments and triggers the pipeline.
- Loads and validates configuration via ConfigManager.
- Scans/collects files and constructs FileInfo objects.
- Creates token-aware ProcessingBuckets via BucketManager.
- Selects an LLM provider through the LLMInterface (OpenAIClient implementation provided).
- Invokes MermaidGenerator to iteratively process buckets with the LLM, merging diagrams and summaries, then writes the final repo.mermaid.
- Optionally integrates with GitHub (Octokit) to commit PR updates (suggested by config and package-lock dependencies).

2) ConfigManager (inferred from tests):
- Conciliates configuration from defaults, YAML file, and CLI arguments; applies a clear override rule (CLI args override file values).
- Reads/writes YAML via fs-extra and yaml libraries; handles file read errors by falling back to defaults.
- Validates required fields and numerical bounds (e.g., max_tokens > 0, temperature ∈ [0,2]).
- Enumerates supported providers and default models (e.g., openai → gpt-4, claude → claude-3-sonnet-20240229, grok → grok-beta).

3) BucketManager (src/core/BucketManager, inferred via tests):
- Partitions FileInfo[] into ProcessingBucket[] based on a max token capacity and a readiness threshold; provides methods to add/remove files, split/merge buckets, and compute utilization and statistics.
- Enforces a hard per-bucket token ceiling (e.g., 400k) to skip pathological files and avoid prompt blowups.
- Tracks and reports skipped files with reasons; can clear summaries.
- Optimizes bucket distribution and reports stats (total buckets, tokens, files, averages, utilization).

4) LLM Abstraction and OpenAIClient (src/llm):
- LLMInterface defines the contract: processBucket, generateSummary, generateMermaid, validateConnection, estimateCost, getAvailableModels, testConnection, etc.
- OpenAIClient extends LLMInterface and uses the openai SDK. It supports both Chat Completions (typical GPT-4) and a newer Responses API path for a hypothetical gpt-5 model.
- Produces a merged unit of work per bucket: summary + Mermaid diagram segment, returning tokens_used when available. Includes robust error handling and cost estimation heuristics based on model pricing.

5) MermaidGenerator (src/output/MermaidGenerator.ts):
- Orchestrates iterative bucket processing with the LLM client, merging accumulated summary and diagram across buckets.
- Sanitizes Mermaid content to escape problematic characters in labels (> < =). Provides a merging strategy:
  - If diagram types match (e.g., flowchart), intelligently merges nodes and edges by extracting sets and recombining them.
  - If diagram types differ, appends as a compound diagram section.
- Writes final output to repo.mermaid, optionally appending a human-readable metadata block (summary, stats, timestamps, skipped files) as HTML comments for GitHub compatibility.
- Performs basic syntax checks (presence of diagram type, balanced brackets/parentheses) and provides processing statistics (completion percentage, average files per bucket, etc.).

Key Design Patterns and Decisions
- Strategy/Bridge for LLM Providers: LLMInterface decouples the orchestration (MermaidGenerator/CLI) from provider specifics (OpenAIClient). This enables future providers (e.g., Anthropic) without changing callers.
- Pipeline Architecture: Clear stages—config loading → file scanning → bucketing → LLM processing → merge/sanitize → output. Each stage is testable and replaceable.
- Token-aware Scaling: BucketManager enforces both soft thresholds (readiness) and hard ceilings to prevent runaway prompts; it also supports merging/splitting for efficient utilization.
- Defensive Output: MermaidGenerator sanitizes content, merges diagrams with minimal duplication, and records skipped files and processing stats in comments.
- Resilience: ConfigManager falls back to defaults on read errors; LLM client surfaces clear, wrapped errors. Integration tests exercise the full CLI without coupling to internals.

External Dependencies and Integration Points
- fs-extra and yaml for configuration and output IO.
- openai SDK for LLM operations; optionally @anthropic-ai/sdk present in dependencies for future expansion.
- glob/commander/ora/chalk suggest a robust CLI experience (progress feedback and filtering), inferred from package-lock.
- @octokit/rest indicates optional GitHub PR integration (config keys branch/commit/pr_title/pr_body), consistent with tests referencing github settings.

Testing Strategy
- Unit tests (BucketManager, ConfigManager) validate functional boundaries, error paths, and correctness of core behaviors (bucketing logic, overrides, validation, skipped files, and stats).
- Integration test constructs a small TypeScript project, runs the CLI via ts-node, and verifies output presence, syntax class, and error handling for empty/specific-file cases. It conditionally requires OPENAI_API_KEY to exercise real LLM flows.

Data Contracts and Types (inferred)
- FileInfo: path, content, size, extension, estimated_tokens.
- ProcessingBucket: files: FileInfo[], total_tokens, summary?, mermaid_content?.
- ProcessingState: progress counters and accumulated summary/mermaid strings.

End-to-end Flow
CLI → ConfigManager (merge/validate) → File scan (glob) → BucketManager (create/optimize, stats, skipped) → MermaidGenerator.processBuckets(LLMInterface.processBucket) → merge/sanitize → MermaidGenerator.generateFinalMermaidFile → repo.mermaid (+ optional GitHub PR).

--- Additional Analysis ---

Architecture overview
The repository provides two orchestration paths to generate architecture summaries and Mermaid diagrams from code:
1) CLI pipeline (src/cli.ts) for local use. 2) GitHub Action workflow (src/github/GitHubActionHandler.ts) that runs in CI, updates/creates a diagram file in a branch, and opens/updates a PR.

Core responsibilities and flow
- Configuration (ConfigManager)
  - Sources: defaults → optional YAML file → CLI args (clear override order: CLI > file > defaults).
  - Validates required fields (e.g., max_tokens > 0, temperature in [0,2]).
  - In TS version, includes colors and additional_instructions that feed LLM prompts and diagram styling; default llm.model is gpt-5 with large context assumptions; token limits default to 100k (TS) but 400k in JS build.
  - Dependencies: fs-extra (IO) and yaml (parse/stringify).

- File discovery (FileDiscovery)
  - Discovers files either: specific list, recursive via glob with ignore patterns (supports wildcards and directory patterns), or single directory non-recursive listing.
  - Normalizes Windows paths and performs a second-pass filter to respect exclude_patterns.
  - Produces FileInfo entries with rough estimated_tokens (content length/4) as a first-pass approximation.
  - Dependencies: fs-extra, glob, path.

- Token-aware bucketing (BucketManager + TokenCalculator)
  - TokenCalculator does a denser, line-aware token estimate (ignores comment/empty lines, accounts for special chars, density per extension) and utilities to estimate prompt/message tokens.
  - BucketManager sorts by size (tokens), packs into ProcessingBucket[] under a capacity and threshold, and enforces a hardLimit to avoid pathological prompts. It can split over-capacity buckets, merge underutilized ones, and maintain skippedFiles. It also supports splitting single large files into chunks.
  - Note: hard limit divergence exists between implementations (TS hardLimit=100k; JS compiled hardLimit=400k). Ensure consistency with chosen model and Config.llm.max_tokens.

- LLM abstraction and OpenAI provider (LLMInterface, OpenAIClient)
  - LLMInterface is a strategy/bridge abstraction providing: processBucket, generateSummary, generateMermaid, validateConnection, estimateCost, getAvailableModels, testConnection. It centralizes prompt creation (system+user), JSON parsing of model responses, and Mermaid cleaning. Prompts embed the color scheme and diagram guidance for consistent, styled output.
  - OpenAIClient implements the interface using the OpenAI SDK. It supports Chat Completions for GPT-4/3.5 and a hypothetical Responses API path for gpt-5. It can list models, estimate cost with model-specific pricing, and validate connectivity.

- Output pipeline (MermaidGenerator, StateManager)
  - MermaidGenerator iteratively processes buckets via the LLM, merges summaries and Mermaid segments, sanitizes and validates Mermaid (basic checks), and writes final repo.mermaid. It can merge across different diagram types defensively and escape problematic characters.
  - StateManager persists progress (accumulated summary/diagram, checkpoints) and provides a simple merge strategy. The CLI uses it; the GitHubAction path does not.

- GitHub integration (GitHubActionHandler, GitHubClient)
  - GitHubActionHandler runs the same core pipeline remotely: tests GitHub connectivity, ensures a target branch, optionally loads existing Mermaid from the branch (to enable incremental merges), runs discovery→bucketing→LLM→merge, writes/updates the diagram via GitHubClient, and creates/updates a PR with labels and comments.
  - GitHubClient wraps Octokit REST to create branches, read/write files, create PRs, add labels/comments, and inspect PRs/commits.

Data contracts
- FileInfo: path, content, size, extension, estimated_tokens.
- ProcessingBucket: files[], total_tokens, optional summary, mermaid_content.
- LLMResponse: summary, mermaid_content, tokens_used.
- ProcessingState: tracking for progress and accumulations.
- Config: file_types, exclude_patterns, llm configuration (provider, model, max_tokens, temperature, additional_instructions), output options, colors, and GitHub settings.

Control and data flow
- CLI path: CLI parses args → ConfigManager merges/validates → FileDiscovery produces FileInfo[] → BucketManager may split large files then create buckets → LLMInterface.processBucket invoked per bucket by MermaidGenerator → Mermaid content sanitized/merged → write repo.mermaid → State saved.
- GitHub Action path: ActionHandler validates GitHub → ensures/creates branch → optionally reads existing Mermaid → runs same file discovery/bucketing/LLM/merge → writes file via GitHub API → creates or updates PR with summary and labels.

Design patterns and decisions
- Strategy/Bridge for LLM providers via LLMInterface. OpenAIClient is a concrete strategy; future providers (Anthropic, xAI) can plug in.
- Pipeline architecture with clear stage boundaries and testability.
- Token-aware scaling and safety: readiness thresholds, capacity ceilings, per-file splitting, and skippedFiles reporting.
- Resilience and DX: defensive reads/writes, helpful warnings, validation, and unit/integration tests.
- Style consistency: prompts instruct the LLM to use classDef coloring and meaningful shapes/labels for diagrams.

Testing
- Unit tests cover ConfigManager, FileDiscovery, TokenCalculator, LLMInterface, and OpenAIClient (mocked SDK), plus dist-tests equivalents.
- Integration test executes the CLI via ts-node on a small TS project, verifying output presence and basic diagram syntax; guarded by OPENAI_API_KEY.

Notable inconsistencies and risks
- Divergent defaults between TS and compiled JS for llm.max_tokens and BucketManager.hardLimit (100k vs 400k) and default llm model choices. Align these to avoid under/over-packing buckets and to match the chosen provider’s context window.
- Default model set to gpt-5 in config while OpenAI availability varies; CLI/test paths handle connection validation.

Resulting system
A modular, token-aware, provider-agnostic pipeline that discovers and batches code, prompts an LLM for an architecture summary and a colored Mermaid diagram, sanitizes and merges outputs across buckets, and writes a final diagram suitable for local use or automated PR updates.

Processing Statistics:
- Total files processed: 42
- Total buckets: 2
- Generated on: 2025-10-05T00:29:30.706Z

-->