flowchart TD
  %% Nodes
  CLI["CLI (src/cli.ts)\n(commander, ts-node)"]:::config
  CFG["ConfigManager\n(load/merge/validate YAML)"]:::config
  YAML["Config YAML (.yml)"]:::config
  DISC["File discovery\n(fs-extra, glob)"]:::core
  FILES["FileInfo[]\n(path, content, ext, size, estimated_tokens)"]:::core
  TOK["TokenCalculator\n(estimate tokens)"]:::core
  BM["BucketManager\n(maxTokens, threshold, hardLimit=400k)"]:::core
  D_HARD{Exceeds\n400k hard limit?}
  SKIP["Skipped Files\n(summary + reasons)"]:::core
  D_READY{Bucket over\nthreshold?}
  BUCKETS["Processing Buckets\n(files + total_tokens)"]:::core
  LLM_IF["LLMInterface\n(create prompts, parse/clean)"]:::llm
  OA["OpenAIClient\n(Chat/Responses API)"]:::llm
  OAPI((OpenAI API)):::llm
  SUMM((Summary text)):::output
  MERM((Mermaid diagram\n.output .mermaid)):::output
  %% Control & Data Flow
  %% Bucketing with decisions
  %% Bucket readiness (utilization)
  %% LLM processing
  %% Iterative refinement with previous context
  SUMM -. previous summary .-> OA
  MERM -. previous diagram .-> OA
  %% Tests (dotted dependencies)
  UT_BM["Unit Test:\nBucketManager.test"]:::tests
  UT_CFG["Unit Test:\nConfigManager.test"]:::tests
  IT_FULL["Integration Test:\nfull-integration.test"]:::tests
  UT_BM -. validates .-> BM
  UT_CFG -. validates .-> CFG
  IT_FULL -. e2e via npx .-> CLI
  %% Styles
  classDef tests fill:#e17055,stroke:#ffffff,stroke-width:2px
  classDef config fill:#fdcb6e,stroke:#ffffff,stroke-width:2px
  classDef core fill:#0984e3,stroke:#ffffff,stroke-width:2px,color:#ffffff
  classDef llm fill:#55efc4,stroke:#ffffff,stroke-width:2px
  classDef output fill:#6c5ce7,stroke:#ffffff,stroke-width:2px,color:#ffffff
  %% Class assignments
  class CLI,CFG,YAML config
  class DISC,FILES,TOK,BM,BUCKETS,SKIP core
  class LLM_IF,OA,OAPI llm
  class SUMM,MERM output
  class UT_BM,UT_CFG,IT_FULL tests
  %% Endpoints
  START((User/CI)):::config
  END((.mermaid file)):::output
  %% CLI and Config
  CLI["CLI (src/cli.ts)\nParse args, orchestrate"]:::config
  CFG["ConfigManager\n(load/merge/validate)"]:::config
  YAML["Config YAML\n(mermaid_generator_config.yaml)"]:::config
  %% Discovery and Tokens
  FD["FileDiscovery\n(glob/fs-extra)\n-> FileInfo[]"]:::core
  TC["TokenCalculator\n(line-based, density-aware)"]:::core
  %% Bucketing
  BM["BucketManager\n(maxTokens, threshold, hardLimit=100k)"]:::core
  SPLIT{File >\n100k tokens?}:::core
  CHUNK["splitLargeFile()\ncreate chunks"]:::core
  SKIP["Skipped Files\n(path,size,reason)"]:::core
  BUCKETS["ProcessingBuckets\n(files,total_tokens)"]:::core
  READY{Bucket\n>= threshold?}:::core
  %% LLM Layer
  LLM_IF["LLMInterface\n(prompts, parse, clean)"]:::llm
  OA["OpenAIClient\n(Chat vs Responses)"]:::llm
  MODEL{model ===\n'gpt-5'?}:::llm
  %% Output
  MG["MermaidGenerator\nprocessBuckets + merge"]:::output
  VAL["validateMermaidSyntax()"]:::output
  META["Append summary & stats\n(HTML comment)"]:::output
  STATE["StateManager\nsaveState/checkpoint"]:::output
  %% GitHub Integration
  GHA["GitHubActionHandler\nWorkflow Orchestrator"]:::config
  GH["GitHubClient\n(Octokit)"]:::config
  BR{Branch exists?}:::config
  PR{PR exists?}:::config
  %% Flow: CLI path
  %% LLM processing inside MG
  %% Output path
  %% GitHub path (alternate orchestrator)
  START -. optional CI .-> GHA
  class START,CLI,CFG,YAML,GHA,GH,BR,PR config
  class FD,TC,BM,SPLIT,CHUNK,SKIP,BUCKETS,READY core
  class LLM_IF,OA,MODEL,OAPI llm
  class MG,VAL,META,STATE,END output
  %% Tests (contextual, not in main flow)
  UT_CFG["Unit: ConfigManager.test"]:::tests
  UT_FD["Unit: FileDiscovery.test"]:::tests
  UT_TC["Unit: TokenCalculator.test"]:::tests
  UT_LLM["Unit: LLMInterface.test"]:::tests
  UT_OA["Unit: OpenAIClient.test"]:::tests
  IT_FULL["Integration: full-integration.test"]:::tests
  UT_FD -. validates .-> FD
  UT_TC -. validates .-> TC
  UT_LLM -. validates .-> LLM_IF
  UT_OA -. validates .-> OA
  IT_FULL -. e2e via CLI .-> CLI
  class UT_CFG,UT_FD,UT_TC,UT_LLM,UT_OA,IT_FULL tests
  CLI --> CFG
  CFG <---> YAML
  CLI --> DISC
  DISC --> FILES
  FILES --> TOK
  TOK --> BM
  BM --> D_HARD
  D_HARD -- Yes --> SKIP
  D_HARD -- No --> BUCKETS
  BUCKETS --> D_READY
  D_READY -- Yes/Process --> OA
  D_READY -- No/Accumulate --> BM
  OA --> OAPI
  OA -- "processBucket /\n generateSummary /\n generateMermaid" --> SUMM
  OA -- Mermaid content --> MERM
  START --> CLI
  CFG <--> YAML
  CLI --> FD
  FD --> BM
  BM --> SPLIT
  SPLIT -- Yes --> CHUNK --> BM
  SPLIT -- No --> BM
  BM -- file > 100k --> SKIP
  BM --> BUCKETS
  BUCKETS --> READY
  READY -- Yes --> MG
  READY -- No --> BM
  MG -->|for each bucket| LLM_IF
  LLM_IF --> OA
  OA --> MODEL
  MODEL -- Yes: Responses API --> OAPI
  MODEL -- No: Chat Completions --> OAPI
  OA --> MG
  MG --> VAL --> META --> END
  MG --> STATE
  GHA --> GH
  GHA -->|load existing .mermaid| MG
  GH --> BR
  BR -- No: create --> GH
  BR -- Yes --> GH
  GHA -->|run processing pipeline\n(FileDiscovery->BucketManager->MermaidGenerator)| MG
  MG -->|final content| GH
  GH --> PR
  PR -- Yes: update --> GH
  PR -- No: create --> GH


<!--
Generated Mermaid Diagram
========================

Summary:
The codebase is a CLI-driven tool that analyzes a repository, partitions files into token-aware buckets, and uses an LLM to generate an architectural summary and a Mermaid diagram. It is organized into four main layers:

1) CLI and Configuration (tools/config)
- CLI (src/cli.ts, invoked via ts-node): Orchestrates the end-to-end flow. It parses CLI arguments (commander), loads/merges configuration, discovers files (glob, fs-extra), and writes outputs. It supports options like file-types, token limits, output path, verbosity, and optional dry runs.
- ConfigManager (src/ConfigManager): Loads YAML configuration (yaml, fs-extra), merges CLI args over file config, provides defaults, validates settings (file types, LLM provider/model, token/temperature bounds), and can write sample configs. It exposes helper methods for supported file types, providers, and default models. Tests show providers include openai, claude, and grok with defaults like gpt-4, claude-3-sonnet-20240229, and grok-beta.

2) Core Processing (business logic)
- BucketManager (src/core/BucketManager): Implements partitioning of files into token-bounded ProcessingBuckets. It:
  - Sorts files by estimated token count (descending) to pack efficiently (first-fit decreasing style).
  - Tracks a configurable maxTokensPerBucket and a hardLimit (400,000 tokens) to guard against oversized buckets/files.
  - Provides operations for bucket readiness, capacity checks, utilization calculations, add/remove files, bucket split/merge, optimization of under/over-utilized buckets, and statistics aggregation.
  - Supports splitting of large single files into chunks when they exceed the hard limit and records skipped files with reasons and summaries.
  - Delegates token estimation to TokenCalculator (src/core/TokenCalculator), which computes per-file token estimates and supports different file extensions.
- TokenCalculator (src/core/TokenCalculator): Used by BucketManager to estimate token counts. Although the implementation is not shown, it is central to all bucket decisions and to large-file splitting.

3) LLM Abstraction and OpenAI implementation
- LLMInterface (src/llm/LLMInterface): An abstraction defining the contract for providers. It is responsible for prompt construction and response parsing/cleaning, exposing methods used by concrete clients.
- OpenAIClient (src/llm/OpenAIClient): A concrete implementation using the openai SDK, supporting both Chat Completions and the newer Responses API pattern (used when model === 'gpt-5'). It provides:
  - processBucket: Generates a combined architectural summary and Mermaid content for a bucket, incrementally incorporating previous summary/diagram.
  - generateSummary and generateMermaid: Specialized calls for isolated summary or diagram generation, using careful system/user prompts focused on architecture and valid Mermaid syntax.
  - validateConnection, getAvailableModels, testConnection: Convenience/diagnostics to verify connectivity and model availability.
  - estimateCost: Rough cost estimation based on model pricing.
  - It expects LLMInterface to supply helpers like createSystemPrompt, createUserPrompt, parseResponse, and cleanMermaidContent.

4) Output and Rendering
- The primary output is a Mermaid diagram file (.mermaid). The tool can also keep/aggregate a textual summary that informs iterative diagram generation. The integration test validates that the CLI produces a non-trivial Mermaid diagram containing recognized diagram keywords.

Data and Control Flow
- The CLI loads configuration via ConfigManager, discovers files via glob/fs-extra, and builds FileInfo objects containing path, content, extension, size, and estimated_tokens.
- BucketManager, with TokenCalculator, groups files into ProcessingBuckets respecting both maxTokensPerBucket and a hard 400k token ceiling. Oversized files are split or skipped with recorded reasons.
- The CLI iterates over buckets and calls OpenAIClient.processBucket, passing previous summary/diagram to iteratively refine the overall architecture representation. It then writes the final Mermaid diagram to disk and optionally includes summary text.

Design Patterns and Notable Decisions
- Strategy pattern for LLM providers: LLMInterface abstracts common behavior; OpenAIClient implements a provider-specific strategy. The config suggests support for additional providers (Anthropic, Grok) can be added as new implementations.
- First-fit decreasing style bucketing with utilization thresholds for readiness and optimization; clear separation of token estimation and bucket orchestration promotes testability.
- Defensive handling of API/model variants (Chat Completions vs Responses API) and token ceilings (per-bucket and hard limit) to prevent failures.
- Robust configuration handling (file overrides CLI defaults, CLI overrides file config; graceful fallbacks when config file is missing or unreadable).

Testing
- Unit tests (BucketManager.test.ts/js): Validate creation, merging, splitting, utilization calculations, large-file handling, and statistics; they emphasize correctness of capacity and hard-limit behaviors.
- Unit tests (ConfigManager.test.js): Validate config load/merge, save, validation, provider/model defaults, and supported file types.
- Integration test: Spins up a fake TS project and executes the CLI via child_process.execSync to verify end-to-end behavior, output presence and plausibility, and failure cases (empty directories, specific files mode).

--- Additional Analysis ---

Overview
The project is a CLI- and GitHub-action-capable tool that analyzes a repository, partitions code into token-aware buckets, and uses an LLM (OpenAI) to produce an architectural summary and a Mermaid diagram. It is organized into cohesive layers with clear responsibilities and composable interfaces.

Architectural Layers and Key Components
1) CLI and Configuration
- CLI (src/cli.ts) orchestrates the workflow: parses args (commander), loads configuration (ConfigManager), discovers files (FileDiscovery), prepares bucketed workloads (BucketManager), invokes LLM processing via MermaidGenerator, writes outputs, saves processing state (StateManager), and optionally integrates with GitHub.
- ConfigManager (src/ConfigManager.ts) merges defaults, YAML config, and CLI args. It validates constraints (file types, LLM settings, output path), exposes supported providers/models, and can save sample configs. Notable defaults: OpenAI provider, model gpt-5, max_tokens 100k (optimized for GPT-5 window), temperature 0.1, with configurable color palette and additional LLM instructions.

2) Core Processing (file I/O and token-aware bucketing)
- FileDiscovery (src/core/FileDiscovery.ts) finds and loads files either recursively (glob) or in a single directory, normalizes ignores (including wildcard patterns), and estimates tokens per file with a rough heuristic (length/4). It returns FileInfo records consumed downstream.
- TokenCalculator (src/core/TokenCalculator.ts) provides a more nuanced token estimation used by BucketManager: per-line tokenization with comment/empty-line filtering, special character weighting, file-type density adjustments, and helpers for prompts/messages.
- BucketManager (src/core/BucketManager.ts) performs FFD-style packing into ProcessingBuckets subject to maxTokensPerBucket and a hard limit (100k tokens) to protect LLM context. It:
  - processFilesWithSplitting: pre-splits oversized files into smaller chunks via splitLargeFile (based on lines and density), to keep chunks under the hard limit.
  - createBuckets: packs sorted files by estimated tokens, respecting a utilization threshold and hard limit; files exceeding the hard limit are skipped with reasons recorded.
  - Provides operations for add/remove files, split/merge buckets, utilization calculation, optimization of under-/over-utilized buckets, and statistics aggregation.

3) LLM Abstraction and OpenAI Implementation
- LLMInterface (src/llm/LLMInterface.ts) defines the provider strategy and prompt contract. It embeds consistent diagram color guidance (tests/config/core/llm/output), constructs system/user prompts, parses provider responses (prefers JSON), and cleans Mermaid content (enforces valid starts, strips code fences).
- OpenAIClient (src/llm/OpenAIClient.ts) concretely implements the provider:
  - processBucket: builds system/user prompts and calls either Chat Completions or the Responses API (model === 'gpt-5'). It parses JSON, cleans Mermaid, and reports estimated token usage.
  - generateSummary and generateMermaid: specialized flows for isolated generation and iterative diagram enhancement.
  - validateConnection, getAvailableModels, testConnection, estimateCost: operational/diagnostic utilities.

4) Output and Rendering
- MermaidGenerator (src/output/MermaidGenerator.ts) coordinates bucket-by-bucket LLM calls, incrementally merges summaries and diagrams, validates Mermaid syntax, and writes the final .mermaid output. It implements an intelligent merge for flowcharts (combining nodes and edges) and supports mixed diagram types via compound composition. Metadata (summary, stats, skipped files) is appended as an HTML comment for GitHub rendering compatibility.
- StateManager (src/output/StateManager.ts) persists/resumes processing state and checkpoints, enabling resumability and progress tracking.

5) GitHub Integration
- GitHubClient (src/github/GitHubClient.ts) wraps Octokit for branch management, file CRUD in repo, PR creation/update, labels, and comments.
- GitHubActionHandler (src/github/GitHubActionHandler.ts) orchestrates end-to-end GitHub workflows: validates connection, ensures branches, loads existing Mermaid, processes the codebase (FileDiscovery -> BucketManager -> MermaidGenerator/LLM), writes updates, and creates/updates PRs with stats/comments.

Data and Control Flow
1) CLI path: CLI parses args -> ConfigManager loads/merges/validates -> FileDiscovery produces FileInfo[] -> BucketManager.processFilesWithSplitting splits oversized files -> BucketManager.createBuckets partitions files -> MermaidGenerator.processBuckets iteratively calls OpenAIClient.processBucket -> MermaidGenerator merges content and writes final .mermaid -> StateManager saves state and stats are reported.
2) GitHub path: GitHubActionHandler validates repo, ensures branch, optionally loads existing .mermaid, then runs the same core processing pipeline, writes the file via GitHubClient, and manages the PR lifecycle.

Notable Decisions and Patterns
- Strategy Pattern for LLM: LLMInterface abstracts common capabilities; OpenAIClient implements provider-specific logic (Chat Completions vs Responses API). Extensibility is evident for additional providers (Anthropic/Grok).
- Capacity Planning via FFD Bucketing: Files are sorted by token count for efficient packing. A hard 100k-token ceiling protects against oversize context windows. Pre-split logic reduces skipped files and improves throughput.
- Incremental Diagram/Summary Merging: MermaidGenerator aggregates context, enabling iterative refinement across buckets and thoughtful merging of flowcharts.
- Defensive I/O and Validation: ConfigManager, MermaidGenerator, FileDiscovery, and GitHubClient handle missing resources and failures gracefully.

Testing
- Unit tests cover ConfigManager, FileDiscovery, TokenCalculator, LLMInterface, OpenAIClient and validate parsing, defaults/overrides, discovery patterns, token math, API error handling, and prompt construction.
- Integration test executes the CLI over a synthetic TS project to validate end-to-end generation and output plausibility.

Key Relationships
- CLI -> ConfigManager -> FileDiscovery -> BucketManager(TokenCalculator) -> MermaidGenerator(LLMInterface/OpenAIClient) -> Output (.mermaid) and StateManager.
- GitHubActionHandler -> GitHubClient for repository interactions and wraps the same core pipeline.

Scalability and Extensibility
- Token-aware splitting and bucketing allow processing large repos progressively.
- Provider strategy enables adding new LLM backends without changing the core pipeline.
- Color-theming and prompt scaffolding ensure consistent, legible diagrams across runs.

Processing Statistics:
- Total files processed: 42
- Total buckets: 2
- Generated on: 2025-10-05T00:21:01.606Z

-->