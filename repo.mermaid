flowchart TD
  cli(("CLI"))
  configMgr["ConfigManager"]
  configFile["config.yml"]
  bucketMgr["BucketManager"]
  typesPB["ProcessingBucket[]"]
  typesPS["ProcessingState"]
  llmIfc["LLMInterface (abstract)"]
  openaiClient["OpenAIClient"]
  openaiSDK["openai SDK"]
  mermaidGen["MermaidGenerator"]
  fsExtra["fs&minus;extra"]
  validator["ValidateMermaid"]
  decisionValid{"Valid"}
  sanitizer["SanitizeMermaid"]
  merger["MergeDiagrams"]
  accumulator["Accumulate Summary &amp; Mermaid"]
  output(("Output"))
  testsUnit["Unit Tests"]
  testsIntegration["Integration Tests"]
  openaiClient -- llmIfc
  decisionValid -- sanitizer
  testsUnit -- bucketMgr
  testsUnit -- configMgr
  testsIntegration -- cli
  testsIntegration -- openaiClient
  classDef tests fill:#e17055,stroke:#ffffff,stroke-width:2px
  classDef config fill:#fdcb6e,stroke:#ffffff,stroke-width:2px
  classDef core fill:#0984e3,stroke:#ffffff,stroke-width:2px
  classDef llm fill:#55efc4,stroke:#ffffff,stroke-width:2px
  classDef output fill:#6c5ce7,stroke:#ffffff,stroke-width:2px
  class cli,typesPB,typesPS,bucketMgr core
  class mermaidGen,validator,sanitizer,merger,accumulator,output output
  class llmIfc,openaiClient,openaiSDK llm
  class configMgr,configFile,fsExtra config
  class testsUnit,testsIntegration tests
  ghAction["GitHubActionHandler"]
  yamlFile["mermaid_generator_config.yaml"]
  fileDiscovery["FileDiscovery"]
  tokenCalc["TokenCalculator"]
  typesIndex["types/index.ts"]
  stateMgr["StateManager"]
  ghClient["GitHubClient (Octokit)"]
  ghRepo(("GitHub Repo"))
  decValid{"Valid config?"}
  branchExists{"Branch exists?"}
  existingPR{"Existing PR?"}
  output(("repo.mermaid"))
  testsIntegration["Integration Test"]
  class cli,fileDiscovery,bucketMgr,tokenCalc,typesIndex,typesPB,stateMgr core
  class configMgr,yamlFile,fsExtra config
  class mermaidGen,output,ghAction,ghClient,ghRepo output
  cli --> configMgr
  configMgr --> configFile
  cli --> bucketMgr
  bucketMgr --> typesPB
  typesPB --> mermaidGen
  typesPS --> mermaidGen
  cli --> openaiClient
  mermaidGen --> llmIfc
  openaiClient --> openaiSDK
  cli --> mermaidGen
  mermaidGen --> validator
  validator --> decisionValid
  decisionValid --> merger
  sanitizer --> merger
  merger --> accumulator
  mermaidGen --> fsExtra
  mermaidGen --> output
  configMgr --> yamlFile
  configMgr --> decValid
  decValid --> fileDiscovery
  cli --> fileDiscovery
  fileDiscovery --> fsExtra
  fileDiscovery --> bucketMgr
  fileDiscovery --> typesIndex
  bucketMgr --> tokenCalc
  bucketMgr --> typesIndex
  mermaidGen --> openaiClient
  mermaidGen --> stateMgr
  mermaidGen --> typesIndex
  ghAction --> ghClient
  ghAction --> fileDiscovery
  ghAction --> bucketMgr
  ghAction --> openaiClient
  ghAction --> mermaidGen
  ghClient --> branchExists
  branchExists --> ghClient
  ghClient --> existingPR
  existingPR --> ghClient
  ghClient --> output
  ghClient --> ghRepo
  testsUnit --> configMgr
  testsUnit --> fileDiscovery
  testsUnit --> bucketMgr
  testsUnit --> tokenCalc
  testsUnit --> openaiClient
  testsUnit --> llmIfc
  testsIntegration --> cli

<!--
Generated Mermaid Diagram
========================

Summary:
Architecture overview

The repository implements a CLI-driven tool that analyzes source code and produces a sanitized, GitHub-compatible Mermaid diagram along with an architectural summary. The system is organized around a clear separation of concerns: core orchestration (CLI and managers), LLM provider abstraction, and output/rendering.

Key components and responsibilities

- Core orchestration and domain
  - CLI (src/cli.ts, referenced in tests): Entry point coordinating the run. It loads configuration, discovers files, buckets them by estimated tokens, selects an LLM provider, and delegates to the output generator. It acts as the composition root.
  - BucketManager (src/core/BucketManager, inferred from tests): Groups files into ProcessingBucket arrays under a soft token limit (configurable) and a hard safety ceiling (~400k tokens). It supports: createBuckets, isBucketReady, capacity checks, add/remove files, split/merge strategies, statistics, and optimizations. It tracks skipped files that exceed hard limits and returns a summary for transparency. This component encapsulates batching heuristics and protects downstream LLM calls from overflows.
  - Types (src/types, inferred): Provides FileInfo, ProcessingBucket, ProcessingState. ProcessingState tracks iterative progress (current bucket index, totals, accumulated summary and Mermaid, etc.).

- LLM abstraction and provider
  - LLMInterface (src/llm/LLMInterface, inferred): An abstraction for code analysis and diagram generation. It defines processBucket and likely shared helpers such as parseResponse and cleanMermaidContent, used by provider implementations.
  - OpenAIClient (src/llm/OpenAIClient.js): Concrete provider extending LLMInterface. It adapts to OpenAI’s APIs:
    - Chat Completions for most models
    - Responses API for gpt-5
    It builds system and user prompts, calls the model, parses output (delegates to base helpers), returns a structured result with summary, sanitized Mermaid, and token usage. It can validate connectivity, list models, perform a test call, and estimate cost. This follows a Strategy/Adapter pattern: LLMInterface defines the contract; OpenAIClient implements provider-specific logic behind a uniform interface.

- Output and rendering
  - MermaidGenerator (src/output/MermaidGenerator.ts): Orchestrates bucket-by-bucket analysis, merges summary text, and merges Mermaid fragments into a single diagram. Core capabilities:
    - processBuckets: Iterates over ProcessingBucket[], calls llmClient.processBucket, aggregates summary and diagram content, updates ProcessingState.
    - Sanitization and GitHub compatibility: sanitizeMermaidContent removes unsupported arrow label syntax (e.g., -->|label|), quotes labels with special characters, escapes reserved characters (&, <, >, =, #, +, -), and normalizes whitespace.
    - Merging: mergeSummaries concatenates sections; mergeMermaidDiagrams tries intelligentDiagramMerge. For flowcharts, it extracts nodes and connections, deduplicates, and rebuilds a coherent flowchart. For heterogeneous diagram types, it creates compound sections.
    - Validation: validateMermaidSyntax performs basic checks: non-empty content, presence of a valid diagram directive (graph/flowchart/classDiagram/sequenceDiagram/stateDiagram/erDiagram/journey/gantt/pie/gitgraph), and balanced brackets/parentheses.
    - Output: buildFinalMermaidContent constructs the final diagram first (to support GitHub renderers) with optional metadata comments. writeMermaidFile persists the diagram using fs-extra.

- Configuration
  - ConfigManager (src/ConfigManager, inferred, tested in dist-tests/unit/ConfigManager.test.js): Manages YAML configuration. It loads defaults when no file exists, merges/overrides with CLI args, validates constraints (file types present, LLM provider/model validity, token/temperature ranges), and can save a fully resolved config. It exposes supported file types/providers and default models per provider.

- Tests
  - Unit tests (tests/unit and dist-tests/unit): Jest-based tests verify BucketManager’s bucketing heuristics, capacity rules, optimization, and skipped files handling; ConfigManager’s load/merge/validate/save logic.
  - Integration test (dist-tests/integration/full-integration.test.js): Assembles a small TypeScript repo in a temp directory, runs the CLI via npx ts-node, and validates that a Mermaid file is produced (when OPENAI_API_KEY is present). This validates end-to-end flow, including file discovery, bucketing, LLM invocation, and diagram output.

Data and control flow

1) CLI discovers files and estimates tokens. Files are passed to BucketManager to form ProcessingBucket[].
2) CLI constructs an LLM provider (e.g., OpenAIClient) via LLMInterface and instantiates MermaidGenerator with this client.
3) MermaidGenerator.processBuckets iterates buckets, calling llmClient.processBucket(bucket, accumulatedSummary, accumulatedMermaid). The provider returns a summary and diagram fragment per bucket.
4) MermaidGenerator merges summaries and diagrams, sanitizes for GitHub Mermaid compatibility, optionally validates, and writes the final repo.mermaid (or configured path).
5) ConfigManager contributes runtime configuration including provider, model, token limits, output path, and tool behavior.

Design patterns and decisions

- Strategy/Adapter: LLMInterface abstracts provider details; OpenAIClient adapts OpenAI APIs.
- Pipeline/orchestration: MermaidGenerator acts as the coordinator for iterative analysis and accumulation.
- Defensive engineering: token thresholds in BucketManager; sanitizeMermaidContent ensures GitHub-compatible Mermaid; validateMermaidSyntax performs basic quality checks.
- Testability: Unit tests isolate managers; Integration test validates CLI, provider wiring, and output.

External dependencies

- openai SDK: Used by OpenAIClient for model listing and generation (chat or responses).
- fs-extra, path: File I/O and safe directory creation for output.
- yaml: Config read/write.
- jest: Testing.

Resulting artifact

- A single Mermaid diagram file (default repo.mermaid) that merges multi-bucket analysis into a consistent, sanitized, and renderable diagram, with optional summary and processing metadata appended as comments.

--- Additional Analysis ---

Architecture and relationships

Entry points
- CLI (src/cli.ts, composition root): Orchestrates a local run. It parses CLI args, loads/validates configuration, discovers files, prepares buckets, initializes the LLM provider, and drives Mermaid generation and state persistence. It can optionally consume an existing Mermaid file to incrementally extend the diagram.
- GitHubActionHandler (src/github/GitHubActionHandler.ts): Composition root for CI. It validates GitHub connectivity, manages branches, reads any existing diagram from the repo, processes the codebase (via the same discovery/bucketing/LLM/generator flow), commits the new diagram, and creates/updates a PR with labels and comments.

Configuration and types
- ConfigManager (src/ConfigManager.ts|.js): Merges defaults + file (mermaid_generator_config.yaml) + CLI args. Validates constraints (provider/model present, token/temperature ranges, output path). TS flavor includes a colors block and additional LLM instructions. The compiled JS variant shows slightly different defaults (e.g., config path and default max_tokens), which is noteworthy if mixing ts-node and transpiled JS.
- Types (src/types): Core domain types FileInfo, ProcessingBucket, ProcessingState, LLMResponse, Config, GitHubConfig, CLIArgs. These types unify the pipeline and support serialization of processing state.

File discovery and token estimation
- FileDiscovery (src/core/FileDiscovery.ts|.js): Finds candidate files either from an explicit list or by scanning recursively (glob) or non-recursively (fs.readdir). Applies exclude patterns robustly (wildcards and directories, Windows-safe). Loads content and estimates tokens.
- TokenCalculator (src/core/TokenCalculator.ts|.js): Provides a more nuanced token estimate than 1 token ≈ 4 chars by removing comments/empty lines, counting word-length-derived tokens, and adding minor weight for punctuation. Also offers density adjustments per extension, prompt/message token estimation, and a recalculation helper.

Bucketing and safeguards
- BucketManager (src/core/BucketManager.ts|.js): Packs FileInfo into ProcessingBuckets respecting soft thresholds and a hard limit. It sorts files by estimated tokens for better packing, decides when a bucket is ready/at capacity, computes utilization, merges/splits buckets, and produces statistics. It tracks and reports skipped files that exceed hard limits, and can split a single oversized file into labeled “Part N” chunks to stay within context windows. Note: there is a hard limit variance between TS (100k) and JS (400k) builds; ensure runtime aligns with your chosen model/context window.

LLM abstraction and OpenAI provider
- LLMInterface (src/llm/LLMInterface.ts|.js): Defines the provider contract (processBucket, generateSummary, generateMermaid, validateConnection, estimateCost) and shared utilities: structured system/user prompts, response parsing (JSON-first fallback to text), and Mermaid cleaning. The TS version embeds color scheme guidance and strict GitHub Mermaid compatibility rules (no arrow labels like -->|label|, escape special characters, wrap labels with special chars in double quotes, supported diagram types only).
- OpenAIClient (src/llm/OpenAIClient.ts): Strategy/Adapter implementing LLMInterface. It switches between Chat Completions API for most models and Responses API for gpt-5. It builds prompts, handles token/temperature, cleans/sanitizes Mermaid, returns tokens used, validates connection (model listing), estimates cost based on model pricing, lists available GPT models, and runs test connections. Errors are wrapped consistently for diagnostics.

Output pipeline and state
- MermaidGenerator (src/output/MermaidGenerator.ts, described in prior context): Coordinates bucket-by-bucket analysis, accumulates summaries, sanitizes and merges Mermaid fragments into a single GitHub-compatible diagram, validates basic Mermaid structure, and writes the final .mermaid file.
- StateManager (src/output/StateManager.ts): Persists ProcessingState to a JSON file to support resumability/checkpoints. It merges summaries/diagrams across buckets, provides progress metrics, and can write/load a checkpoint with bucket metadata.

GitHub integration
- GitHubClient (src/github/GitHubClient.ts): Thin Octokit wrapper for repo queries, branch management, file create/update (base64), PR create/update, labeling, and commenting.
- GitHubActionHandler: Wires GitHubClient with the core pipeline. Workflow: test connection → get repo info → ensure branch → optionally load existing Mermaid from target branch → run the analysis pipeline → create/update the diagram file on the branch → create or update a PR and comment.

Data and control flow
1) Configuration: CLI args → ConfigManager (merge defaults + YAML + args) → validation → colors/instructions passed into LLMInterface.
2) Discovery and tokenization: FileDiscovery loads and estimates tokens → TokenCalculator can refine estimations.
3) Bucketing: BucketManager builds ProcessingBucket[] with capacity and hard-limit guards, tracks skipped files and optionally splits large files.
4) Analysis: MermaidGenerator iterates buckets, calling LLMInterface.processBucket (OpenAIClient) → returns summary + GitHub-safe Mermaid fragment + usage stats.
5) Merging and output: MermaidGenerator merges and sanitizes content, validates basic syntax, and writes repo.mermaid. StateManager persists progress; GitHubActionHandler may commit and PR the artifact.

Design patterns and decisions
- Strategy/Adapter for LLM providers; orchestration/pipeline for processing; defensive engineering for token windows (thresholds, splitting, skipped files); sanitization for GitHub Mermaid compatibility; state persistence for robustness; CI integration via Octokit abstraction.

Tests
- Unit tests cover ConfigManager load/merge/validate/save, FileDiscovery (specific, recursive, non-recursive, error handling), TokenCalculator estimation APIs, LLMInterface utilities, and OpenAIClient behavior (success/empty/error, validate/model list/cost/test connection).
- Integration tests construct a small repo, invoke the CLI via ts-node, and assert generation of a Mermaid diagram, with variants for empty directory and specific file subsets.

Notable considerations
- Keep the TS vs JS config defaults aligned (config path, llm.max_tokens, hard limits) to avoid surprising behavior across ts-node vs transpiled runs.
- For GitHub rendering, adhere to Mermaid restrictions: avoid edge labels like -->|label|, quote labels with special characters, and ensure diagram types and parentheses/brackets are valid.


Processing Statistics:
- Total files processed: 42
- Total buckets: 2
- Generated on: 2025-10-05T04:05:36.588Z

-->